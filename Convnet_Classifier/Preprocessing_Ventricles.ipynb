{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import skimage\n",
    "from skimage import transform\n",
    "import nibabel\n",
    "import os\n",
    "from shutil import copyfile\n",
    "copyfile(src, dst)\n",
    "\n",
    "Centres=['KCL_T1','MAN_T1', 'NIJ_T1']\n",
    "\n",
    "Demogs=os.path.join(filepath, '/mnt/storage/home/robert/Convnet_Classifier/EUAIMS_ventricle_deviations.csv')\n",
    "Demographics=pd.read_excel(Demogs, index_col='subjects')\n",
    "\n",
    "test_loc = '/mnt/storage/home/robert/Convnet_Classifier/Convnet_Classifier/data/Ventricles_Regression/'\n",
    "train_loc = '/mnt/storage/home/robert/Convnet_Classifier/Convnet_Classifier/data/Ventricles_Regression/'\n",
    "\n",
    "with open (test_loc+'/test_subs.txt', 'rb') as f:\n",
    "    test_ids = f.readlines()\n",
    "with open (train_loc+'/train_subs.txt', 'rb') as f:\n",
    "    train_ids = f.readlines()\n",
    "    \n",
    "    \n",
    "#Copy files to Convnet location\n",
    "for centre in Centres:\n",
    "    filepath=('/mnt/storage/home/robert/AIMS/FDA_Ready/V01/'+centre)\n",
    "    files = os.file.listdir(filepath)\n",
    "    \n",
    "    c_files = list(filter(lambda k: '_Warped.nii.gz' in k, files))\n",
    "\n",
    "    for file in c_files:\n",
    "        if file[:12] in test_ids:\n",
    "            copyfile(file, test_loc)\n",
    "        elif file[:12] in train_ids:\n",
    "            copyfile(file, train_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Caudate_ROIS = list(filter(lambda k: 'Caudate' in k, ROIS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'997038807877_T1_brain_MNI_Warped.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects=('/mnt/storage/home/robert/3D-Convnet-for-Alzheimer-s-Detection/data/EUAIMS_MNIWARP_Order_by_ID_Subjects.txt')\n",
    "with open (subjects, 'r') as f:\n",
    "    subs=f.read().splitlines()\n",
    "subs=[int(x) for x in subs]\n",
    "Labels = Demographics[Demographics.index.isin(subs)]\n",
    "cols={'t1_group'}\n",
    "Labels=Labels[cols]\n",
    "\n",
    "ssdata_dir='/mnt/storage/home/robert/AIMS/EUAIMS_MNIWARP/Harmonized_Maps'\n",
    "nldata_dir='/mnt/storage/home/robert/3D-Convnet-for-Alzheimer-s-Detection/data/TD'\n",
    "aldata_dir='/mnt/storage/home/robert/3D-Convnet-for-Alzheimer-s-Detection/data/ASD'\n",
    "\n",
    "#labeling and object formation\n",
    "\n",
    "for file in os.listdir(ssdata_dir):\n",
    "    netdata=[]                                              #will be used for numpy object\n",
    "    try:\n",
    "        img = nibabel.load(os.path.join(ssdata_dir, file))  # loading the image\n",
    "        img = img.get_fdata()                           # accessing image array\n",
    "        img = skimage.transform.resize(img, (106, 106, 120))#resizing the image to dimensions(106,106,120)\n",
    "        id = file.partition('.')\n",
    "        label=Labels.get_value(int(id[0]), 't1_group') #getting the label\n",
    "        if (label == 1 | 3):\n",
    "            labelar = np.array([1, 0])\n",
    "            netdata.append([img, labelar])                          #one hot encoding and saving numpy object\n",
    "            np.save(os.path.join(nldata_dir, id[0] ), netdata)\n",
    "        elif (label == 2 | 4):\n",
    "            labelar = np.array([0, 1])\n",
    "            netdata.append([img, labelar])\n",
    "            np.save(os.path.join(aldata_dir, id[0] ), netdata)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "#normalisation\n",
    "\n",
    "totalnum=[]         #total number of pixels in the image\n",
    "mean=[]             #mean of the pixels in the image\n",
    "nummax=[]           #maximum value of pixels in the image\n",
    "for file in os.listdir(aldata_dir):\n",
    "    img = np.load(os.path.join(aldata_dir,file))\n",
    "    mean.append(np.mean(img[0][0]))\n",
    "    totalnum.append((img[0][0].shape[0]*img[0][0].shape[1]*img[0][0].shape[2]))\n",
    "    nummax.append(np.max(img[0][0]))\n",
    "for file in os.listdir(nldata_dir):\n",
    "    img = np.load(os.path.join(nldata_dir, file))\n",
    "    mean.append(np.mean(img[0][0]))\n",
    "    totalnum.append((img[0][0].shape[0]*img[0][0].shape[1]*img[0][0].shape[2]))\n",
    "    nummax.append(np.max(img[0][0]))\n",
    "nummean=np.vdot(mean,totalnum)/np.sum(totalnum)           #mean value for the full dataset\n",
    "nummax=np.max(nummax)                                     #max value for the full dataset\n",
    "\n",
    "for file in os.listdir(aldata_dir):\n",
    "    img = np.load(os.path.join(aldata_dir,file))\n",
    "    img[0][0]=(img[0][0]-nummean)/nummax                 #normalisation(x-mean/max value)\n",
    "    np.save(os.path.join(aldata_dir,file),img)\n",
    "for file in os.listdir(nldata_dir):\n",
    "    img = np.load(os.path.join(nldata_dir, file))\n",
    "    img[0][0] =(img[0][0] - nummean) / nummax\n",
    "    np.save(os.path.join(nldata_dir,file),img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
